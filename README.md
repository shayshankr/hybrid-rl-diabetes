# Hybrid Reinforcement Learning for Personalized Diabetes Care

**MSc in Artificial Intelligence â€“ National College of Ireland (2025)**  
**Author:** Shayshank Rathore  


---

## ğŸ“– Project Overview
This project develops a **Hybrid Reinforcement Learning (RL) framework** to support **personalized diabetes care**.  
It combines two core innovations:

- **Reward Decomposition** â†’ Splits overall reward into clinically interpretable components:  
  - Glycaemic control (Time-in-Range)  
  - Hypoglycaemia penalties (safety)  
  - Treatment cost (insulin use burden)  

- **Meta-Learning** â†’ Enables the RL agent to adapt quickly to new patient profiles using limited data, improving **sample efficiency** and personalisation.

A **custom Gymnasium environment** was built using the UCI AIMâ€™94 Diabetes dataset to simulate patient trajectories and evaluate treatment decisions. The primary RL agent is **Proximal Policy Optimization (PPO)**, benchmarked against a simple **rule-based controller**.

---

## ğŸ—ï¸ Repository Structure
```
hybrid-rl-diabetes/
â”‚
â”œâ”€â”€ notebooks/        # Implementation notebooks
â”‚   â”œâ”€â”€ thesis-code.ipynb
â”‚   â”œâ”€â”€ project-code.ipynb
â”‚   â”œâ”€â”€ msc-project.ipynb
â”‚
â”œâ”€â”€ data/             # Dataset (UCI Diabetes â€“ 70 patients)
â”‚   â””â”€â”€ diabetes-data.zip
â”‚
â”œâ”€â”€ outputs/          # Model artefacts
â”‚   â”œâ”€â”€ all_outputs_20250829.zip
â”‚   â””â”€â”€ 1Output.zip
â”‚
â”œâ”€â”€ docs/             # Reports and manuals
â”‚   â”œâ”€â”€ MSc_Research_Project_Report_Shayshank_Rathore.pdf
â”‚   â””â”€â”€ MSc_Research_Project_Config_Man.pdf
â”‚
â”œâ”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ LICENSE           # MIT License
â””â”€â”€ .gitignore        # Ignore checkpoints, cache, outputs
```

---

## âš™ï¸ How to Run
1. **Clone the repository**
   ```bash
   git clone https://github.com/<username>/hybrid-rl-diabetes.git
   cd hybrid-rl-diabetes
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run notebooks**
   - Upload to [Kaggle Notebooks](https://www.kaggle.com/code)
   - Attach the dataset (`diabetes-data.zip`)
   - Enable **GPU (T4)**
   - Execute notebooks top to bottom  

4. **Outputs**
   - Models â†’ `outputs/models/`  
   - Logs & CSVs â†’ `outputs/logs/`  
   - Plots â†’ `outputs/plots/`  

Full setup details are available in the **Configuration Manual** under `/docs/`.

---

## ğŸ“Š Key Results
On held-out test patients:
- **PPO vs Rule-based baseline**  
  - âœ… Eliminated **hypoglycaemic steps** (0 vs 1)  
  - âœ… Reduced **insulin use by ~86%** (1.34 vs 9.72 units/step)  
  - **Meta-learning adaptation**:  
  - Small but consistent per-task reward gains (**âˆ† reward â‰ˆ +2.28**)  
  - Suggests feasibility of rapid personalisation  

Overall, PPO learned a **conservative, safe, insulin-sparing policy**. Future iterations will focus on **finer action spaces**, **reward rebalancing**, and **stochastic dynamics**.

---

## ğŸ“Œ Features
- âœ… Custom **Gymnasium** environment for diabetes treatment simulation  
- âœ… **Reward decomposition** for transparent decision-making  
- âœ… **Meta-learning** (Reptile-style fine-tuning) for per-patient adaptation  
- âœ… PPO vs Rule-based baseline comparison  
- âœ… Clinical + RL metrics: TIR, insulin use, hypo/hyper events, reward curves  

---

## ğŸ“š References
- [UCI AIMâ€™94 Diabetes Dataset](https://doi.org/10.24432/C5T59G)  
- [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3)  
- [Farama Gymnasium](https://github.com/Farama-Foundation/Gymnasium)  
- Rathore, S. *MSc Project Report: Hybrid Reinforcement Learning for Personalized Diabetes Care*, NCI, 2025.  

---

## ğŸ“ License
This project is licensed under the **MIT License** â€“ see [LICENSE](LICENSE) for details.  
